{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50adabea",
   "metadata": {},
   "source": [
    "# 03 – Word2Vec + Clustering\n",
    "Trains Word2Vec on lemmas, builds document vectors, reduces with PCA, clusters,\n",
    "and evaluates (Silhouette, Davies–Bouldin, Dunn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from smclust.data_loader import load_messages\n",
    "from smclust.preprocessing import preprocess_df\n",
    "from smclust.embeddings import train_word2vec, doc_vector\n",
    "from smclust.reduce import pca_reduce\n",
    "from smclust.cluster import kmeans_cluster, dbscan_cluster\n",
    "from smclust.metrics import silhouette, davies_bouldin, dunn_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb94b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_messages()\n",
    "df = preprocess_df(df)\n",
    "\n",
    "tokenized = df[\"lemmas\"].tolist()\n",
    "w2v = train_word2vec(tokenized, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "X = np.vstack([doc_vector(toks, w2v) for toks in tokenized])\n",
    "\n",
    "Xp, pca = pca_reduce(X, n_components=50, random_state=42)\n",
    "\n",
    "labels_km, _ = kmeans_cluster(Xp, n_clusters=8, random_state=42)\n",
    "labels_db, _ = dbscan_cluster(Xp, eps=0.7, min_samples=5)\n",
    "\n",
    "print(\"KMeans  -> sil:\", silhouette(Xp, labels_km),\n",
    "      \" db:\", davies_bouldin(Xp, labels_km),\n",
    "      \" dunn:\", dunn_index(Xp, labels_km))\n",
    "\n",
    "print(\"DBSCAN  -> sil:\", silhouette(Xp, labels_db),\n",
    "      \" db:\", davies_bouldin(Xp, labels_db),\n",
    "      \" dunn:\", dunn_index(Xp, labels_db))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
